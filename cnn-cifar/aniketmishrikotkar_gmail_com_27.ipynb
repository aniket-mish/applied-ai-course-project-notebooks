{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "aniketmishrikotkar@gmail.com_27",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvJDYPqgetpt",
        "colab_type": "text"
      },
      "source": [
        "### CNN on CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OahnRphGqghG",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "    1.Please visit this link to access the state-of-art DenseNet code for reference - DenseNet - cifar10 notebook link\n",
        "    2.You need to create a copy of this and \"retrain\" this model to achieve 90+ test accuracy.\n",
        "    3.You cannot use Dense Layers (also called fully connected layers), or DropOut.\n",
        "    4.You MUST use Image Augmentation Techniques.\n",
        "    5.You cannot use an already trained model as a beginning points, you have to initilize as your own\n",
        "    6.You cannot run the program for more than 300 Epochs, and it should be clear from your log, that you have only used 300 Epochs\n",
        "    7.You cannot use test images for training the model.\n",
        "    8.You cannot change the general architecture of DenseNet (which means you must use Dense Block, Transition and Output blocks as mentioned in the code)\n",
        "    9.You are free to change Convolution types (e.g. from 3x3 normal convolution to Depthwise Separable, etc)\n",
        "    10.You cannot have more than 1 Million parameters in total\n",
        "    11.You are free to move the code from Keras to Tensorflow, Pytorch, MXNET etc.\n",
        "    12.You can use any optimization algorithm you need.\n",
        "    13.You can checkpoint your model and retrain the model from that checkpoint so that no need of training the model from first if you lost at any epoch while training. You can directly load that model and Train from that epoch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wVIx_KIigxPV",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dsO_yGxcg5D8",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "num_classes = 10\n",
        "epochs = 100\n",
        "l = 6\n",
        "num_filter = 35\n",
        "compression = 1"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mB7o3zu1g6eT",
        "colab": {}
      },
      "source": [
        "# Load CIFAR10 Data\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes) "
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5HqpmKCkbin",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.10)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2QWm-TYR75T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dense block\n",
        "def denseblock(input, num_filter = 12):\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l): \n",
        "        BatchNorm = layers.BatchNormalization()(temp)\n",
        "        relu = layers.Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
        "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp\n",
        "\n",
        "#transition block\n",
        "def transition(input, num_filter = 12):\n",
        "    global compression\n",
        "    BatchNorm = layers.BatchNormalization()(input)\n",
        "    relu = layers.Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
        "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    return avg\n",
        "\n",
        "#output layer\n",
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = layers.BatchNormalization()(input)\n",
        "    relu = layers.Activation('relu')(BatchNorm)\n",
        "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    conv = layers.Conv2D(num_classes, (1,1), use_bias=False ,padding='same')(AvgPooling)\n",
        "    maxpooling = layers.GlobalMaxPooling2D()(conv)\n",
        "    output = layers.Activation('softmax')(maxpooling)\n",
        "    return output"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN1-KcF1p2HD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input = layers.Input(shape=(img_height, img_width, channel,))\n",
        "First_Conv2D = layers.Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
        "\n",
        "First_Block = denseblock(First_Conv2D, num_filter)\n",
        "First_Transition = transition(First_Block, num_filter)\n",
        "\n",
        "Second_Block = denseblock(First_Transition, num_filter)\n",
        "Second_Transition = transition(Second_Block, num_filter)\n",
        "\n",
        "Third_Block = denseblock(Second_Transition, num_filter)\n",
        "Third_Transition = transition(Third_Block, num_filter)\n",
        "\n",
        "Last_Block = denseblock(Third_Transition,  num_filter)\n",
        "output = output_layer(Last_Block)"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRErWTznNSX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://keras.io/api/preprocessing/image/#imagedatagenerator-class\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.125,\n",
        "    height_shift_range=0.125,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    zoom_range=0.1,\n",
        "    shear_range=0.2,\n",
        ")"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVXjsAf4JRpP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b457e0f2-3e32-4a69-cbd7-1ea38e3aab6a"
      },
      "source": [
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_21 (InputLayer)           [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_597 (Conv2D)             (None, 32, 32, 35)   945         input_21[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_584 (BatchN (None, 32, 32, 35)   140         conv2d_597[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_585 (Activation)     (None, 32, 32, 35)   0           batch_normalization_584[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_598 (Conv2D)             (None, 32, 32, 35)   11025       activation_585[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_504 (Concatenate)   (None, 32, 32, 70)   0           conv2d_597[0][0]                 \n",
            "                                                                 conv2d_598[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_585 (BatchN (None, 32, 32, 70)   280         concatenate_504[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_586 (Activation)     (None, 32, 32, 70)   0           batch_normalization_585[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_599 (Conv2D)             (None, 32, 32, 35)   22050       activation_586[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_505 (Concatenate)   (None, 32, 32, 105)  0           concatenate_504[0][0]            \n",
            "                                                                 conv2d_599[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_586 (BatchN (None, 32, 32, 105)  420         concatenate_505[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_587 (Activation)     (None, 32, 32, 105)  0           batch_normalization_586[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_600 (Conv2D)             (None, 32, 32, 35)   33075       activation_587[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_506 (Concatenate)   (None, 32, 32, 140)  0           concatenate_505[0][0]            \n",
            "                                                                 conv2d_600[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_587 (BatchN (None, 32, 32, 140)  560         concatenate_506[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_588 (Activation)     (None, 32, 32, 140)  0           batch_normalization_587[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_601 (Conv2D)             (None, 32, 32, 35)   44100       activation_588[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_507 (Concatenate)   (None, 32, 32, 175)  0           concatenate_506[0][0]            \n",
            "                                                                 conv2d_601[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_588 (BatchN (None, 32, 32, 175)  700         concatenate_507[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_589 (Activation)     (None, 32, 32, 175)  0           batch_normalization_588[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_602 (Conv2D)             (None, 32, 32, 35)   55125       activation_589[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_508 (Concatenate)   (None, 32, 32, 210)  0           concatenate_507[0][0]            \n",
            "                                                                 conv2d_602[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_589 (BatchN (None, 32, 32, 210)  840         concatenate_508[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_590 (Activation)     (None, 32, 32, 210)  0           batch_normalization_589[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_603 (Conv2D)             (None, 32, 32, 35)   66150       activation_590[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_509 (Concatenate)   (None, 32, 32, 245)  0           concatenate_508[0][0]            \n",
            "                                                                 conv2d_603[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_590 (BatchN (None, 32, 32, 245)  980         concatenate_509[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_591 (Activation)     (None, 32, 32, 245)  0           batch_normalization_590[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_604 (Conv2D)             (None, 32, 32, 35)   8575        activation_591[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_79 (AveragePo (None, 16, 16, 35)   0           conv2d_604[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_591 (BatchN (None, 16, 16, 35)   140         average_pooling2d_79[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_592 (Activation)     (None, 16, 16, 35)   0           batch_normalization_591[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_605 (Conv2D)             (None, 16, 16, 35)   11025       activation_592[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_510 (Concatenate)   (None, 16, 16, 70)   0           average_pooling2d_79[0][0]       \n",
            "                                                                 conv2d_605[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_592 (BatchN (None, 16, 16, 70)   280         concatenate_510[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_593 (Activation)     (None, 16, 16, 70)   0           batch_normalization_592[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_606 (Conv2D)             (None, 16, 16, 35)   22050       activation_593[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_511 (Concatenate)   (None, 16, 16, 105)  0           concatenate_510[0][0]            \n",
            "                                                                 conv2d_606[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_593 (BatchN (None, 16, 16, 105)  420         concatenate_511[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_594 (Activation)     (None, 16, 16, 105)  0           batch_normalization_593[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_607 (Conv2D)             (None, 16, 16, 35)   33075       activation_594[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_512 (Concatenate)   (None, 16, 16, 140)  0           concatenate_511[0][0]            \n",
            "                                                                 conv2d_607[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_594 (BatchN (None, 16, 16, 140)  560         concatenate_512[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_595 (Activation)     (None, 16, 16, 140)  0           batch_normalization_594[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_608 (Conv2D)             (None, 16, 16, 35)   44100       activation_595[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_513 (Concatenate)   (None, 16, 16, 175)  0           concatenate_512[0][0]            \n",
            "                                                                 conv2d_608[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_595 (BatchN (None, 16, 16, 175)  700         concatenate_513[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_596 (Activation)     (None, 16, 16, 175)  0           batch_normalization_595[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_609 (Conv2D)             (None, 16, 16, 35)   55125       activation_596[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_514 (Concatenate)   (None, 16, 16, 210)  0           concatenate_513[0][0]            \n",
            "                                                                 conv2d_609[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_596 (BatchN (None, 16, 16, 210)  840         concatenate_514[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_597 (Activation)     (None, 16, 16, 210)  0           batch_normalization_596[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_610 (Conv2D)             (None, 16, 16, 35)   66150       activation_597[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_515 (Concatenate)   (None, 16, 16, 245)  0           concatenate_514[0][0]            \n",
            "                                                                 conv2d_610[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_597 (BatchN (None, 16, 16, 245)  980         concatenate_515[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_598 (Activation)     (None, 16, 16, 245)  0           batch_normalization_597[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_611 (Conv2D)             (None, 16, 16, 35)   8575        activation_598[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_80 (AveragePo (None, 8, 8, 35)     0           conv2d_611[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_598 (BatchN (None, 8, 8, 35)     140         average_pooling2d_80[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_599 (Activation)     (None, 8, 8, 35)     0           batch_normalization_598[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_612 (Conv2D)             (None, 8, 8, 35)     11025       activation_599[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_516 (Concatenate)   (None, 8, 8, 70)     0           average_pooling2d_80[0][0]       \n",
            "                                                                 conv2d_612[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_599 (BatchN (None, 8, 8, 70)     280         concatenate_516[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_600 (Activation)     (None, 8, 8, 70)     0           batch_normalization_599[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_613 (Conv2D)             (None, 8, 8, 35)     22050       activation_600[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_517 (Concatenate)   (None, 8, 8, 105)    0           concatenate_516[0][0]            \n",
            "                                                                 conv2d_613[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_600 (BatchN (None, 8, 8, 105)    420         concatenate_517[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_601 (Activation)     (None, 8, 8, 105)    0           batch_normalization_600[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_614 (Conv2D)             (None, 8, 8, 35)     33075       activation_601[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_518 (Concatenate)   (None, 8, 8, 140)    0           concatenate_517[0][0]            \n",
            "                                                                 conv2d_614[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_601 (BatchN (None, 8, 8, 140)    560         concatenate_518[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_602 (Activation)     (None, 8, 8, 140)    0           batch_normalization_601[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_615 (Conv2D)             (None, 8, 8, 35)     44100       activation_602[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_519 (Concatenate)   (None, 8, 8, 175)    0           concatenate_518[0][0]            \n",
            "                                                                 conv2d_615[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_602 (BatchN (None, 8, 8, 175)    700         concatenate_519[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_603 (Activation)     (None, 8, 8, 175)    0           batch_normalization_602[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_616 (Conv2D)             (None, 8, 8, 35)     55125       activation_603[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_520 (Concatenate)   (None, 8, 8, 210)    0           concatenate_519[0][0]            \n",
            "                                                                 conv2d_616[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_603 (BatchN (None, 8, 8, 210)    840         concatenate_520[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_604 (Activation)     (None, 8, 8, 210)    0           batch_normalization_603[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_617 (Conv2D)             (None, 8, 8, 35)     66150       activation_604[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_521 (Concatenate)   (None, 8, 8, 245)    0           concatenate_520[0][0]            \n",
            "                                                                 conv2d_617[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_604 (BatchN (None, 8, 8, 245)    980         concatenate_521[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_605 (Activation)     (None, 8, 8, 245)    0           batch_normalization_604[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_618 (Conv2D)             (None, 8, 8, 35)     8575        activation_605[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_81 (AveragePo (None, 4, 4, 35)     0           conv2d_618[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_605 (BatchN (None, 4, 4, 35)     140         average_pooling2d_81[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_606 (Activation)     (None, 4, 4, 35)     0           batch_normalization_605[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_619 (Conv2D)             (None, 4, 4, 35)     11025       activation_606[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_522 (Concatenate)   (None, 4, 4, 70)     0           average_pooling2d_81[0][0]       \n",
            "                                                                 conv2d_619[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_606 (BatchN (None, 4, 4, 70)     280         concatenate_522[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_607 (Activation)     (None, 4, 4, 70)     0           batch_normalization_606[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_620 (Conv2D)             (None, 4, 4, 35)     22050       activation_607[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_523 (Concatenate)   (None, 4, 4, 105)    0           concatenate_522[0][0]            \n",
            "                                                                 conv2d_620[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_607 (BatchN (None, 4, 4, 105)    420         concatenate_523[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_608 (Activation)     (None, 4, 4, 105)    0           batch_normalization_607[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_621 (Conv2D)             (None, 4, 4, 35)     33075       activation_608[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_524 (Concatenate)   (None, 4, 4, 140)    0           concatenate_523[0][0]            \n",
            "                                                                 conv2d_621[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_608 (BatchN (None, 4, 4, 140)    560         concatenate_524[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_609 (Activation)     (None, 4, 4, 140)    0           batch_normalization_608[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_622 (Conv2D)             (None, 4, 4, 35)     44100       activation_609[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_525 (Concatenate)   (None, 4, 4, 175)    0           concatenate_524[0][0]            \n",
            "                                                                 conv2d_622[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_609 (BatchN (None, 4, 4, 175)    700         concatenate_525[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_610 (Activation)     (None, 4, 4, 175)    0           batch_normalization_609[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_623 (Conv2D)             (None, 4, 4, 35)     55125       activation_610[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_526 (Concatenate)   (None, 4, 4, 210)    0           concatenate_525[0][0]            \n",
            "                                                                 conv2d_623[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_610 (BatchN (None, 4, 4, 210)    840         concatenate_526[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_611 (Activation)     (None, 4, 4, 210)    0           batch_normalization_610[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_624 (Conv2D)             (None, 4, 4, 35)     66150       activation_611[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_527 (Concatenate)   (None, 4, 4, 245)    0           concatenate_526[0][0]            \n",
            "                                                                 conv2d_624[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_611 (BatchN (None, 4, 4, 245)    980         concatenate_527[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_612 (Activation)     (None, 4, 4, 245)    0           batch_normalization_611[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_82 (AveragePo (None, 2, 2, 245)    0           activation_612[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_625 (Conv2D)             (None, 2, 2, 10)     2450        average_pooling2d_82[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_1 (GlobalM (None, 10)           0           conv2d_625[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_613 (Activation)     (None, 10)           0           global_max_pooling2d_1[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 970,900\n",
            "Trainable params: 963,060\n",
            "Non-trainable params: 7,840\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxiLhGoOCjyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's train the model using Adam\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urDEu2PqNVzH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8ceca200-8bff-49fa-d6fd-c53957ba87fe"
      },
      "source": [
        "datagen.fit(x_train)\n",
        "\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "history = model.fit(datagen.flow(x_train, y_train,\n",
        "                                   batch_size=batch_size),\n",
        "                                   epochs=epochs,\n",
        "                                   validation_data=(x_val, y_val),\n",
        "                                   workers=4)"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1407/1407 [==============================] - 44s 31ms/step - loss: 1.5598 - accuracy: 0.4303 - val_loss: 1.2470 - val_accuracy: 0.5570\n",
            "Epoch 2/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 1.1240 - accuracy: 0.6007 - val_loss: 1.4426 - val_accuracy: 0.5568\n",
            "Epoch 3/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.9169 - accuracy: 0.6790 - val_loss: 1.0549 - val_accuracy: 0.6628\n",
            "Epoch 4/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.7968 - accuracy: 0.7218 - val_loss: 0.8773 - val_accuracy: 0.7060\n",
            "Epoch 5/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.7101 - accuracy: 0.7527 - val_loss: 0.8849 - val_accuracy: 0.7084\n",
            "Epoch 6/100\n",
            "1407/1407 [==============================] - 44s 31ms/step - loss: 0.6529 - accuracy: 0.7734 - val_loss: 0.7550 - val_accuracy: 0.7516\n",
            "Epoch 7/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.6022 - accuracy: 0.7912 - val_loss: 0.7638 - val_accuracy: 0.7606\n",
            "Epoch 8/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.5684 - accuracy: 0.8026 - val_loss: 0.6398 - val_accuracy: 0.7920\n",
            "Epoch 9/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.5325 - accuracy: 0.8145 - val_loss: 0.4875 - val_accuracy: 0.8342\n",
            "Epoch 10/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.5021 - accuracy: 0.8267 - val_loss: 0.5199 - val_accuracy: 0.8284\n",
            "Epoch 11/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.4775 - accuracy: 0.8361 - val_loss: 0.6173 - val_accuracy: 0.7948\n",
            "Epoch 12/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.4528 - accuracy: 0.8418 - val_loss: 0.5361 - val_accuracy: 0.8292\n",
            "Epoch 13/100\n",
            "1407/1407 [==============================] - 43s 30ms/step - loss: 0.4313 - accuracy: 0.8497 - val_loss: 0.5496 - val_accuracy: 0.8200\n",
            "Epoch 14/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.4216 - accuracy: 0.8556 - val_loss: 0.4487 - val_accuracy: 0.8526\n",
            "Epoch 15/100\n",
            "1407/1407 [==============================] - 43s 30ms/step - loss: 0.3993 - accuracy: 0.8623 - val_loss: 0.5580 - val_accuracy: 0.8158\n",
            "Epoch 16/100\n",
            "1407/1407 [==============================] - 43s 30ms/step - loss: 0.3893 - accuracy: 0.8651 - val_loss: 0.4457 - val_accuracy: 0.8596\n",
            "Epoch 17/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.3716 - accuracy: 0.8707 - val_loss: 0.4907 - val_accuracy: 0.8486\n",
            "Epoch 18/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.3668 - accuracy: 0.8737 - val_loss: 0.3681 - val_accuracy: 0.8780\n",
            "Epoch 19/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.3495 - accuracy: 0.8784 - val_loss: 0.5308 - val_accuracy: 0.8366\n",
            "Epoch 20/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.3409 - accuracy: 0.8804 - val_loss: 0.4576 - val_accuracy: 0.8566\n",
            "Epoch 21/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.3234 - accuracy: 0.8876 - val_loss: 0.3971 - val_accuracy: 0.8728\n",
            "Epoch 22/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.3174 - accuracy: 0.8890 - val_loss: 0.4468 - val_accuracy: 0.8614\n",
            "Epoch 23/100\n",
            "1407/1407 [==============================] - 44s 31ms/step - loss: 0.3111 - accuracy: 0.8918 - val_loss: 0.4712 - val_accuracy: 0.8572\n",
            "Epoch 24/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.3017 - accuracy: 0.8939 - val_loss: 0.4253 - val_accuracy: 0.8648\n",
            "Epoch 25/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.2947 - accuracy: 0.8982 - val_loss: 0.3885 - val_accuracy: 0.8716\n",
            "Epoch 26/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.2797 - accuracy: 0.9010 - val_loss: 0.3471 - val_accuracy: 0.8880\n",
            "Epoch 27/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.2786 - accuracy: 0.9024 - val_loss: 0.4155 - val_accuracy: 0.8712\n",
            "Epoch 28/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.2668 - accuracy: 0.9074 - val_loss: 0.4853 - val_accuracy: 0.8530\n",
            "Epoch 29/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.2651 - accuracy: 0.9069 - val_loss: 0.4977 - val_accuracy: 0.8516\n",
            "Epoch 30/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.2535 - accuracy: 0.9120 - val_loss: 0.4316 - val_accuracy: 0.8716\n",
            "Epoch 31/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.2525 - accuracy: 0.9125 - val_loss: 0.3962 - val_accuracy: 0.8772\n",
            "Epoch 32/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.2428 - accuracy: 0.9148 - val_loss: 0.3874 - val_accuracy: 0.8792\n",
            "Epoch 33/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.2354 - accuracy: 0.9176 - val_loss: 0.3338 - val_accuracy: 0.8938\n",
            "Epoch 34/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.2376 - accuracy: 0.9171 - val_loss: 0.4561 - val_accuracy: 0.8652\n",
            "Epoch 35/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.2254 - accuracy: 0.9217 - val_loss: 0.4075 - val_accuracy: 0.8748\n",
            "Epoch 36/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.2271 - accuracy: 0.9209 - val_loss: 0.3547 - val_accuracy: 0.8882\n",
            "Epoch 37/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.2152 - accuracy: 0.9247 - val_loss: 0.3428 - val_accuracy: 0.8926\n",
            "Epoch 38/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.2151 - accuracy: 0.9254 - val_loss: 0.4297 - val_accuracy: 0.8766\n",
            "Epoch 39/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.2107 - accuracy: 0.9261 - val_loss: 0.4019 - val_accuracy: 0.8774\n",
            "Epoch 40/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.2084 - accuracy: 0.9260 - val_loss: 0.3556 - val_accuracy: 0.8948\n",
            "Epoch 41/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.2005 - accuracy: 0.9278 - val_loss: 0.4907 - val_accuracy: 0.8636\n",
            "Epoch 42/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.2011 - accuracy: 0.9303 - val_loss: 0.4285 - val_accuracy: 0.8722\n",
            "Epoch 43/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.1950 - accuracy: 0.9307 - val_loss: 0.4361 - val_accuracy: 0.8706\n",
            "Epoch 44/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.1941 - accuracy: 0.9320 - val_loss: 0.4149 - val_accuracy: 0.8812\n",
            "Epoch 45/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.1865 - accuracy: 0.9340 - val_loss: 0.3903 - val_accuracy: 0.8942\n",
            "Epoch 46/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.1845 - accuracy: 0.9355 - val_loss: 0.3928 - val_accuracy: 0.8880\n",
            "Epoch 47/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.1852 - accuracy: 0.9355 - val_loss: 0.4679 - val_accuracy: 0.8708\n",
            "Epoch 48/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.1786 - accuracy: 0.9365 - val_loss: 0.4435 - val_accuracy: 0.8728\n",
            "Epoch 49/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.1782 - accuracy: 0.9372 - val_loss: 0.3928 - val_accuracy: 0.8804\n",
            "Epoch 50/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.1746 - accuracy: 0.9376 - val_loss: 0.3441 - val_accuracy: 0.8956\n",
            "Epoch 51/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.1690 - accuracy: 0.9414 - val_loss: 0.3434 - val_accuracy: 0.8992\n",
            "Epoch 52/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.1694 - accuracy: 0.9404 - val_loss: 0.3433 - val_accuracy: 0.8992\n",
            "Epoch 53/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.1631 - accuracy: 0.9422 - val_loss: 0.3310 - val_accuracy: 0.8972\n",
            "Epoch 54/100\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.1648 - accuracy: 0.9408 - val_loss: 0.3657 - val_accuracy: 0.8932\n",
            "Epoch 55/100\n",
            "1407/1407 [==============================] - 43s 30ms/step - loss: 0.1615 - accuracy: 0.9432 - val_loss: 0.4171 - val_accuracy: 0.8810\n",
            "Epoch 56/100\n",
            "1407/1407 [==============================] - 43s 30ms/step - loss: 0.1580 - accuracy: 0.9439 - val_loss: 0.3392 - val_accuracy: 0.9042\n",
            "Epoch 57/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1597 - accuracy: 0.9446 - val_loss: 0.3719 - val_accuracy: 0.8960\n",
            "Epoch 58/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1510 - accuracy: 0.9458 - val_loss: 0.3690 - val_accuracy: 0.8980\n",
            "Epoch 59/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1546 - accuracy: 0.9456 - val_loss: 0.4174 - val_accuracy: 0.8858\n",
            "Epoch 60/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1507 - accuracy: 0.9475 - val_loss: 0.3418 - val_accuracy: 0.9006\n",
            "Epoch 61/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1435 - accuracy: 0.9496 - val_loss: 0.4218 - val_accuracy: 0.8872\n",
            "Epoch 62/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1468 - accuracy: 0.9495 - val_loss: 0.3535 - val_accuracy: 0.9006\n",
            "Epoch 63/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1417 - accuracy: 0.9496 - val_loss: 0.3235 - val_accuracy: 0.9082\n",
            "Epoch 64/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1421 - accuracy: 0.9512 - val_loss: 0.3520 - val_accuracy: 0.8976\n",
            "Epoch 65/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1368 - accuracy: 0.9507 - val_loss: 0.4018 - val_accuracy: 0.8898\n",
            "Epoch 66/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1307 - accuracy: 0.9533 - val_loss: 0.4092 - val_accuracy: 0.8960\n",
            "Epoch 67/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1361 - accuracy: 0.9511 - val_loss: 0.4575 - val_accuracy: 0.8808\n",
            "Epoch 68/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1360 - accuracy: 0.9520 - val_loss: 0.3921 - val_accuracy: 0.8974\n",
            "Epoch 69/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1313 - accuracy: 0.9540 - val_loss: 0.3645 - val_accuracy: 0.9034\n",
            "Epoch 70/100\n",
            "1407/1407 [==============================] - 42s 29ms/step - loss: 0.1285 - accuracy: 0.9548 - val_loss: 0.4001 - val_accuracy: 0.8864\n",
            "Epoch 71/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1279 - accuracy: 0.9536 - val_loss: 0.3783 - val_accuracy: 0.8986\n",
            "Epoch 72/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1256 - accuracy: 0.9562 - val_loss: 0.3773 - val_accuracy: 0.8958\n",
            "Epoch 73/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1261 - accuracy: 0.9542 - val_loss: 0.4888 - val_accuracy: 0.8736\n",
            "Epoch 74/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1216 - accuracy: 0.9578 - val_loss: 0.3939 - val_accuracy: 0.9006\n",
            "Epoch 75/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1202 - accuracy: 0.9575 - val_loss: 0.3301 - val_accuracy: 0.9058\n",
            "Epoch 76/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1197 - accuracy: 0.9572 - val_loss: 0.3299 - val_accuracy: 0.9056\n",
            "Epoch 77/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1210 - accuracy: 0.9570 - val_loss: 0.4327 - val_accuracy: 0.8938\n",
            "Epoch 78/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1173 - accuracy: 0.9586 - val_loss: 0.3827 - val_accuracy: 0.8944\n",
            "Epoch 79/100\n",
            "1407/1407 [==============================] - 43s 30ms/step - loss: 0.1148 - accuracy: 0.9581 - val_loss: 0.3589 - val_accuracy: 0.9006\n",
            "Epoch 80/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1172 - accuracy: 0.9585 - val_loss: 0.4043 - val_accuracy: 0.8946\n",
            "Epoch 81/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1133 - accuracy: 0.9599 - val_loss: 0.3927 - val_accuracy: 0.8990\n",
            "Epoch 82/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1118 - accuracy: 0.9603 - val_loss: 0.3562 - val_accuracy: 0.9038\n",
            "Epoch 83/100\n",
            "1407/1407 [==============================] - 43s 30ms/step - loss: 0.1132 - accuracy: 0.9610 - val_loss: 0.4305 - val_accuracy: 0.8910\n",
            "Epoch 84/100\n",
            "1407/1407 [==============================] - 43s 30ms/step - loss: 0.1087 - accuracy: 0.9617 - val_loss: 0.3475 - val_accuracy: 0.9100\n",
            "Epoch 85/100\n",
            "1407/1407 [==============================] - 43s 30ms/step - loss: 0.1086 - accuracy: 0.9622 - val_loss: 0.3525 - val_accuracy: 0.9090\n",
            "Epoch 86/100\n",
            "1407/1407 [==============================] - 43s 30ms/step - loss: 0.1075 - accuracy: 0.9617 - val_loss: 0.3863 - val_accuracy: 0.9014\n",
            "Epoch 87/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1069 - accuracy: 0.9628 - val_loss: 0.4241 - val_accuracy: 0.8906\n",
            "Epoch 88/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1043 - accuracy: 0.9631 - val_loss: 0.3318 - val_accuracy: 0.9092\n",
            "Epoch 89/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1021 - accuracy: 0.9633 - val_loss: 0.3892 - val_accuracy: 0.9040\n",
            "Epoch 90/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1061 - accuracy: 0.9629 - val_loss: 0.3370 - val_accuracy: 0.9090\n",
            "Epoch 91/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1028 - accuracy: 0.9629 - val_loss: 0.3795 - val_accuracy: 0.8990\n",
            "Epoch 92/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1024 - accuracy: 0.9633 - val_loss: 0.3417 - val_accuracy: 0.9108\n",
            "Epoch 93/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1014 - accuracy: 0.9633 - val_loss: 0.3696 - val_accuracy: 0.9032\n",
            "Epoch 94/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.0982 - accuracy: 0.9655 - val_loss: 0.3835 - val_accuracy: 0.9002\n",
            "Epoch 95/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.0965 - accuracy: 0.9660 - val_loss: 0.3907 - val_accuracy: 0.9004\n",
            "Epoch 96/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.0967 - accuracy: 0.9654 - val_loss: 0.3900 - val_accuracy: 0.9060\n",
            "Epoch 97/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.0978 - accuracy: 0.9659 - val_loss: 0.3974 - val_accuracy: 0.9068\n",
            "Epoch 98/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.0968 - accuracy: 0.9658 - val_loss: 0.3736 - val_accuracy: 0.9052\n",
            "Epoch 99/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.0955 - accuracy: 0.9664 - val_loss: 0.3802 - val_accuracy: 0.9138\n",
            "Epoch 100/100\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.0965 - accuracy: 0.9664 - val_loss: 0.3801 - val_accuracy: 0.9010\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H56_rLuXlEzr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "a9bb2cac-a902-4f71-ef00-22a173d77381"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#val accuracy\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fc58aedf1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3dd3hUVfrA8e+bTHoCCSQECB1Beq+igKIuKmJFUNfe21p+u7bdtey6vVhWV8W1sRZUbMhaKYoFgYD03pMA6b1OkvP740waJGECGSbkvp/nmSdz79y5c+4M3Pee95x7jhhjUEop5VwB/i6AUkop/9JAoJRSDqeBQCmlHE4DgVJKOZwGAqWUcjgNBEop5XA+CwQi8oqIpInIhgZeFxF5RkR2iMg6ERnhq7IopZRqmC9rBK8BUxt5/Rygj+dxM/C8D8uilFKqAT4LBMaYpUBWI5tcAMwx1o9AtIh08lV5lFJK1c/lx89OAJJqLSd71h04dEMRuRlbayAiImJkv379jksBlVKqtVi1alWGMSauvtf8GQi8ZoyZDcwGGDVqlElMTPRziZRS6sQiInsbes2fvYZSgK61lrt41imllDqO/BkI5gNXe3oPjQNyjTGHpYWUUkr5ls9SQyLyNjAZiBWRZOBRIAjAGPMC8ClwLrADKAKu81VZlFJKNcxngcAYc/kRXjfAHb76fKWUUt7RO4uVUsrhNBAopZTDaSBQSimH00CglFIOp4FAKaUcTgOBUko5nAYCpZRyOA0ESinlcCfEoHNKKXWiMcawISWPNck5ZBeWkVVYRml5Be0jQoiLCqFNmIv8knJyi9zkl5YjQECAYAzkFrvJLiwju6iM8kpDpTFUVhpum9ybqYOaf7R+DQRKKccrKitnd0YhucVuikorKCwrp6y8EneFwV1RSbArgMgQF5EhLoJdAYhAgAgl7gpyi93kFbspLa+s3t+ujEIWbU4lNa+0el1UqIsQVyBZhaVUmrqfH+KyyRljwGBoGxZMu4ggosOCCQ0KIECEABGCXb5J4mggUEqd8MorKikoLSe32E1afilpeaWk5ZdwMK+E1NwSUvNKyS4qI6fITUFpOVGhLmLCg4kMdZGSXUxKTnGzlic8OJBJfeM4s38843u3JzYypPokXlFpyCosI7/ETZuwINqEBvnsBO8tDQRKqePCXVGJMXh10ssqLGPV3mxyi920iwgiJjyYoMAA8kvKyStxsz+nmI3789i4P4+9mYUUlVXUu5/gwADi24YQHxVK13bhDOkSRGRIEAWlbrIK7ZX8yO4xzBzdld5xkbSLCCYyxEVYcCAhrgCCXQG4AoSyikoKSsrJLy3HXV6JASqNIcQVQNuwINqEBREaFIh4Pjc0KJCgwPqPMzBAiIuy6aGWQgOBUqrZFJSWs2J3Jn3jo+gSEw5AbpGb2d/u5NXv91BUVkFQoBAe7CI8ONDzcBEaFECIy55892YVsSOt4IifFRsZzIDObRnfqz1tw4KICnXRJiyIuKgQOnge7SKCEZEj7ssrbZtnNy2RBgKlFJWVhvJK0+DVujGGran5JO7JZn9OMftziskpdtOjfQT9O0XRPiKEzzYc5LMNB6qvzvvGRzKsazSfbThIfkk55w3pRP+OURSWVVBUWk5RWYXnUU6Ju5KisnKyCivpEhPGRcMTGN2jHfFtQsgucpNVWIq7wtiTfWjNyb7ZTvIOp4FAKQfJLiyjoLScNmFBRIW42J5WwAerk/nwpxRyi92c1ieWswbEM7BzWw7mlpCcXcSmA3l8sy29uuHTFSB0ig4lKiSI5buyKHbbE39kiIsLhnXmZwM7siOtgCVb0/h4zX4m9o3jvrP60r9Tm6Mqc/f2zXb4qgEaCJRqBZKyivh+RwZ5JW4EQcTmsCsqbUPq7oxCVu/LZk9mUfV7RGwvFVeAMPnkOBKiw1i4OY2Fm9Pq7LtNqItT+8QyuW8HxvduT+foMAID7JV4ZaVhX1YR+3OKGd4thrDgQAAmn9yBG0/rhTFGr9pPABoIlDoBlbgrWL47i2+2pvP1tjR2pRc2un1sZAgjukUzc3Q32kcGk+fp8tg+MoTzhnQiNtI2XD423bDpQB57M4voHB1Gl5gw2jeSZw8IEHrERtAjNqLe1zUInBg0ECjVgpSVV5JeUEpaXgkHc0vYllrAttR8dmcUEhgghAXZK+61yTmUltv+7WN7tuPKsd2Z1DeOjm1DMcZgsP3cXQG2/3lQoHh1UhYRBnZuy8DOrbhlVB1GA4FSx8H+nGKyCss4qUMkoZ6T+d7MQhasO8CPuzKr+71nF7nrvE8EurULp5fnirvEXYm7opIrxnZjYt84xvVsX52OUepoaSBQqpmsS87hb19sJSmriK7twunRPoJidwXLd2eSlGVvWHIFCCd1iMQVKGxIyQNgYOc2dG8fzqgeMZ7eMKF0iAqhY9tQesVFEB6s/02Vb+m/MKWOQnGZHVqgoNRNdpGb/y7by/y1+2kfEczYXu1Iyirmo6QUAgOEsT3bcd0pPenQJoTNB/LYkJJHYWk5D5/bj/OGdCYhOszfh6McTgOBUh7GGPbnlrAhJZcSdwXG2OEADuaVsDezkH1ZRaTmlZKeX0pBaXmd94YGBXDn6Sdxy6ReRIUGVe/v0Lz8tCGdj9vxKOUtDQTK0fZmFrJ0Wzrf7chg9b4c0vNL690uLiqE7u3CGdi5DXFRIcRGhlSPVRMZEsighLZ0iAqt8x7tMaNOFBoIVKuXVVhm0zgl5WQUlrL5QB6b9uexNjmnOnffJSaMU0+KZVjXaIZ0aUubsCA7LLDYcWEiQvS/imq99F+3apWSsoqYv3Y/89fsZ2tq/mGvd4kJY1Dnttx0Wi9O6xNHj/bhegWvHEsDgTrhVPW1T88vpbC0HFeA4AoU0vPL+HFXJst2Zlaf/Ed1j+Hhc/sRFxVCZEgQ0eFB9I2Pom1YkJ+PQqmWQwOBatEKS8t57Yc9rNqbzcFcO758VmFZg9uHBgUwukc7LhqRwHmDO9G1XfhxLK1SJyYNBKrFWJ+cS26xm45tQ4mLDOGjNSn8a/F2MgrK6NcxioToMIZ1iyY+KpQObezok5EhLioqDe5KU91oG+LSG6yUagoNBMrvcovcPPG/Tby3Kvmw18b1asdLV/djeLcYP5RMKWfQQKCOq53pBXy67gAGiA4Pwhh4dskOsgrLuH1ybyb2jatOAQ3s3IZTT4rVRlylfEwDgfK5vBI3X25M5d2VSazYk1U9/HGV/p3a8Oq1oxmUoAOdKeUPGghUs0rLKyEtv5ScIjcpOUV8uTGVb7dnUFZRSY/24TwwtR+XjEwgJjyYnCI3+SVuurULx9XA/K5KOVbaFph/F0z9E3QZ5dOP0kCgmkVRWTm/+2QTc1cm1VnfuW0oV43vzrmDOzKiW0ydNE9Lm8BbtVLGwIG1EBELUZ0hwAcXHbu+hoJ0GDKjefZXlAVvz4Ls3fDpL+HGxb4pt4cGAtVkxhgyCsqICAkkLCiQDSl53D33J3ZnFnLjqT0Z3bMd0WFBtI8MoXdchOb4/W33UoiMh7iT/V0S/9j8Cbx7lX0eGAJxfWHa09BlZP3bV5Tbk3D8QDj91+AKtusLM+DL30DHwTD+jprtC9LgnauhNNc+Rt94bOWtcMN710JeCoy9DZY/Dxveb74gUw8NBKpJUvNK+OV7a/l2ewYAwYEBlFdW0iEqlLduHMf43jrBbIuy4X2YdwNEdYLbl0FYtL9LZK/Qt30BecngCrWPk6ZAmI96hv30X1sTmPQryNoFGz+CNy+B6z6DDv0P337nYtjxlX3sXgqXvgKZO+Cj26EwDdbOhU7DoMcEu/1Xj4K7CHqcBv/7JYS0PfJJOycJEl+2f/NSoCTXBpiuY2ztZfc3cOHzMGQW7P0eFv0O+p8PQaGN7/coiandatfcOxeZCjwNBAL/Mcb8+ZDXuwOvAHFAFvBzY8zhfQhrGTVqlElMTPRRiVVjPl1/gIc/XE+pu5JbJvUiNCiQnCI3rgDhxtN6Eh0e7O8intiy98L3T8Hpv4GIZgio2xfaK9sO/SB1Ewy9HC587uj3V5IHqRuh+/hj2EcufHIPbPyg7vrOw+HGRRBwDPeAHNwA+5bZK/KqWmj+Qfhnf5hwD5z5qF2XtRtemWq3uf5ziOlRdz/vXg17voNz/wYL7oXyMigvhg4D4Pyn4cNb7FX7rd9B+hZ45Wdw6r0w6QF441JI+hFmvGZP3PXZuQTmXQ+ledC2C7TpAiGRsP8nKEi124y/E372B/t81zcwZzqc9TuYcPdRfz0issoYU29jg89qBCISCDwHnAUkAytFZL4xZlOtzf4OzDHGvC4iZwB/Aq7yVZlU0+zNLOSbbemsScphXXIuO9IKGNqlLU/OHEavuEh/F6/1WTEbEl+BtM1w1UfeXf0VZdkr6UPTb/t+hHd+boPANQvgh2fg23/AwAuhz1lNL1vWbnhrJmRstSfso2m8TFllT4A5STDlERh2JZSX2ivv//0frJ4Do66r/73lZVBRCiFRh79WUQ7fPwlf/wUq3RDeDgZdYl9b9y6YShh2Rc327XrCVR/Cq+fAnAvhhi8hsoN9rTATtnwKY262++gy2pYtti+c8Vv7m1z8Erx8NvzvPkjfBm0SYOKvICgMLn8bXj/ffvcJo+x+Bl7oOYYSWPUaLHzM7u/GhdC+d025jIGcfbb20Wtyzfpek6DP2bD0HzD8Knt8zcxnNQIRGQ88Zoz5mWf5IQBjzJ9qbbMRmGqMSRKbSM41xrRpbL9aI/CtEncFX2w8yNwVSSzblQlAbGQwQ7pEM+GkWK4e350g7eFTvwq3PfGkbqxZFxkHvafYan9jbSXGwL9G2BNjXgoMuhQu+U/Ne4w5/P3Ze+C5sTD5QXtFWqUkD54ZbtNA131uy1BeCi9OslfkTU0R7fsR5l4BlRVQWQ79psHFL9Yte8pqm1NvKHiV5sM/+kFoNFz6MnQbV/f9r50HaZvgrtWHn+hykuDNGfZqedZbdWskaVvgo9tg/2oYeDFkbLNX2ncmQmAwPH8KBEfYk+6hkhPt5550Jsx8w36/P74Anz8At/1gj6ch3/wVlniu2Ge8XnOyBygtgDVv2sCeuePw9w64EC54ztYCvJW22R7LlEfh1Hu8f18tfqkRAAlA7S4kycDYQ7ZZC1yMTR9dBESJSHtjTGbtjUTkZuBmgG7duvmswE63cFMqj87fSEpOMV3bhfHLs/tywbAEusSEaYNvYyrKYf278M1f7Mk5KAIkADBQVmCvACPjof90e+UYFX/4PjK22/z1ef+A4hxY/HsIb28fOxbCwXVwxTt1rxS/f9peZX77TxhxTc0JdNlzUJQBV75ngwCAKwQu/Df850x7JXvxf+r2QqmsAHfx4SenXd/Am5faFMYV78HyF2D16/CzP9akr1bPgU9+YWsmQ2bByGsOz73v+d5+F7PerBsEwJ6Az/07vHCqzYWf/1TNawfW2SDgLrbHN2c6TH8WBkyHpX+D75+xtYRLX4VBF9v8/n8vghUv2Rx+2iY475/1/25dRsHpD8NXj8DGD+3717xh8/+NBQGAU++DpOU2yAy4oO5rIZEw9hYYfRPsWgxJK22Dc2AIRHe1/w6a+v+pQ3+4/ktIaKCB+xj5u7H4l8CzInItsBRIASoO3cgYMxuYDbZGcDwL6AQ70gr482dbWLg5lT4dInntutFM7BNHQICPTv75ByEi7tjywYdKXgXJK2HcrU1/rzG2UbDrGFu9b+p737oMdi6CjkPg8neg78/q5qh3LobtX9q0wNq37dX7+DvqftbWT+3fvudAm842oKx4ERCbPw9rB5/eD7d9D4FBdr8/vQE9J9myf/cknP1727Nl2bP25JQwom5ZE0bAGb+BRY/bdMbZv7fr8w7Yk21ZPtyxsqaXDMCSP0JUR5sOCm9n8+8rX4Kf5tjjKM6xJ+/OwyGmJ6z8jw0W18yHnhNr9rP7G3si7HrotaBH/AB78vzxeRsoAlyQm2xP9qHRcMMXNpi+ezV8eDN89VtbQxh6hT2OiFi7n95n2BrY0r/Cwan2Mwdd3PDvN+4OGwQ+/ZWtJR1cb4PSkQS64Mp59nlDJ/WAAFvbOOnMI+/PG11HN89+6uHLQJACdK213MWzrpoxZj+2RoCIRAKXGGNyfFgmhe3+uWpvNp9vOMjirWnsSi8kLCiQB8/px/UTehLs8mHqJ2MHPD8ezvkLjLr+yNuXFdkr3HY9YfClDW/3+YM2EAyd1fSeMevesQ2AXUbbE3lTGmr3LbNBYPLDMOn+w08KUR1tfnrYFZC5E778rb3aX/cO3PwNBHtGR936GXQaCm0T7PK0p2DgRXZdRKzNW8+9HFa+bIPdsmdtmub8p2xufMVsGHcb/PCs7cFy+m/qL++p90LefttmENnBnjTfnGF7w1SU2ZrN8J/bbQ+stQ2fZ/+hprbRoZ/tHZP4CpzyC5siKcqEqz6wZS1Ih2dHwpq36waCXd9At7GNB9rJD9peTh/eUrOu0zC4fC606WSXf/4BfHY/pCTa1Fntz6hy9u9t7WLdO/Y7bKw3UqDLpmlenARzr/QEjksa3r62VlRL9mUgWAn0EZGe2AAwC7ii9gYiEgtkGWMqgYewPYiUj2QXlvHhTym8vWIf29MKCA4MYFzv9lw9rjvnDO5EfBvfdE2r49t/2BPOjkVHDgTbPY2IOXttl7y+U+vPqx5cD8kr7POUxKZdgblLYPETEN3d7uflM+2VXu1GvMZ895RN35xy15FPDO17w+VvweYF8M6Vnh5CD9ur+KTl9kRYJdBlu1RWOfkc6HU6fP1He3wrX7EnrHa94PSH7An0f7+0aaRhV9i+8vURsUG4MN32iQ/6k02t3LgQPr7DppuGXmGvZpfPhqDwmsBQZfSN8N418OO/ba1lxNU2CIBNRfU9B7Z9ZttMAoNscEjbaBtbGxPaFm74yv7eER1srTG8Xd3v1RVcN3VUn/iB9jv46Q17LEcSPxAm/hK+/pNtZ/BBY2xL57NAYIwpF5E7gS+w3UdfMcZsFJHfAYnGmPnAZOBPImKwqaE7GtyhajJjDPNWJfP1tnQ2pOSyN7MIgKFdo/nrJUM4d0gnIhubgnHnYpujPcrGqcNk77FXaQFBtm90ZWXDd0v+7/9smiG2L/zsT/DFQ7DmLRh78+HbJr5q+6JXlEHSiqYFgpUvQW4SXP0xuMJsd8uXz7KNrA2dTKukboLtX9jaQHAT5j3oP82exL9/2p5kd38LGHuyb4gITP2zbTB8dSq4C22eGmz3x5HX2mMJDIZJDza8H7ApuYtn25x9QSrMetvmrifcA+/fYE/iXcfB+vfsCfXQGla/8+x9CV/+xgboKY8cfnzr5trfuNdkmxaCuu0bDYnpbh/H6uwnbD7d238Lp95nG9kPDXoO4dM2AmPMp8Cnh6x7pNbzecA8X5bBqYwx/P3LrTy3ZCcJ0WEMTmjLZaO6MqlvnHeDu1VW2rxp5g77Hz+2z7EX6run7Elo8oM2r5y+uf5GuX0/2iAw6gY7zoorxPY7//HfMPqGum0LpfmeFMDF9oo+abn35SnOhqV/t+mRXpPtuhu+gtmT4Lt/wkUvNP7+75+2DcNjbvL+M6uc+bhN93z5W5viaZNg2xga06Gf/azlL8DJ59m8epWJv7Lfw8hr7En9SFwhh+e4B1xof5fvnrJBqaLUdn88VGCQDTxf/wkmP1CTn6/Se4oNqpsX1ASCkDY2zXO8hMV4l3qs4gqGqX/0XXlaOO0H2Eo9tXA7zy3ZyeVjuvHt/afzwlUjueP0k7wf4XPnopqubyv/c+wFyk2xXeqG/9x2jQR7086hjIGFj9uGwbOfsCcsgHG323FXtn1ed/v18+yV7ajrbWNvcqLtAeONb/9pu1Oe9XjNutiTYMhM2PCB7aPfkJwk2DDPnniPJpUQ3dXWtDZ9ZO+yPfkc73LOkx+0DcFTDkmzRMXD3WttgPGWSN3PDHTZFFfyCvvd9JxYN9jUNv4O28OpvkARHG7TWlsW2AuKXd9Aj1Pt/lWLpIGglckqLONvX2zh6UXbuWxUF/5w4aCj6/2z/AWI7Ggb29a8Za+8j8UP/7In6An32Kp/2271B4IdC2HfD/YKt3a6pf90aNsVlv27Zp0x9jb9+MG2K2DXsTYopG06fL+Hyt4Ly1+0J/2Og+u+NvoGezW85s2G37/Mc4fu+GPIZp7yC3tMle7G00K1hcXAZXPqHxohvN2x98QadqVt8yjLhzG3NLxdSJRtKwhsYO7n/tMh/4CtyeXs9S4tpPxGA8GJLGklVFZSUFrO0wu3M/3Z7xj5xFc8t2QnF49I4E8XDzm6IJC+zZ6QR99gu9eV5tm0Q1OVFcLWz20j5qpXbY+eqvxvjwk2h1z7hsbKStu1Mbq77RdfW6DLXn3u/c72ZgF7p+rB9fZuVBFbI4Ajp4eMsf3eA1y2O+Wh4gfaHHniK7ZMh9r6me2lM2Sm7V9/tILDbe+g3mfYnjgtQXA4TH4Iuk/wPjjVp+/Z9vv9ypMJ7jmpecqnfELraieqpBXw8llsnvQ8Ny7vyP7cYkZ0i+GeKX2Z2DeWYV2jj/4msBWzbaPjyGttz41Ow+wNOqNu8L7L3Oo58NkDtitjULg92dU+6XafYPvUp2+pubrd9KE9sV80u25f9iojroav/wxvXmaXC1IhOBKGeJZjetjeJkkrGx8BctVrdtjg8/7RcD599A3wwU2w+2tb9iq7v4V3r7G9ZM75i3ffRWP6nGkfLcmYm46u3aO2sBibWtq52NYsnTry6QlCA8EJKm/zYtoAXy76ipDoa5h363hGdq8nV11RbtMF3p7AS3JtKmjQpTXjr4y5GT6+3aZyeh7hyrW81AaAVa/aq8BT74Xup9Tk+qtUjdy45zsbCEoLbENlhwEN3y8QFm3z+Vv+Z/vbt+lirzyrxp+pqhU0ViPI2Wd7u/ScCCMbaUwccIG9N2HlyzWBIGU1vH25vafh5+/XP+6NqtFvmg0EPSe2qj73rZEGghNM4p4sXv1+DzO3/o+JAfCzuGxuuf00QoPqyQ2XFcGzo+wV85mPefcBq16zXRPH1soPD7oYvvy1rSk0FgiKc+xwBMkrbQA447cN56xjetqeMnu/t1efn91v8/bXfNJ4nvtIV6tdx9pGyoI0G8h2LoZv/lYzxO/qOTY1NP1fjU/04QqxA3z98Iy9aWr3UtubKaqTHbDMgX3Nm6z/+bbhf8B0f5dEHYEGghNEaXkFj3+yibeW7yMmVPinawdUQr+AZKgvCIDN6+el2G6OVXepNmbD+/Y/bu8p0LlWV7+gMHtSXPacHdogqmP97189xwaBqnFfGiNi00O7voa179iG2UkPHLnGcSRV49gkrbD3ILx7jT2pH1jrGbIBmxI6dOjh+oy6zjZyL/kDdBho79wdc7MdAkIdWWQHeGCPT2fWUs1DA8EJ4EBuMbe9sZo1STncMrEX9wwsJOTVImjXG7J22rtjDx310Rg7bkuHAfYu0gX32j7yDV1t//QmzL8Tuo2Hy14//PUR19ir4zVvwWn31b+P9e/am3iOFASq9Jhg3zP/Tuh2Cky837v3NabTUNu+sf0L21AZGAw3LbFX8mkb7fAKfad6t6+YHnDrt3asm6qhH1TTaBA4Ieiv1IJlF5bx8ne7Of9f37E9NZ/nrxzBQ+f2J2y/Jwc+6no71nrGtsPfvHOxHTv+lF/YO3NTVtleMPVJfMW2AfScZG8yqi/3HXuSvYL/6b91e/pUSdtiG3oHX+b9AXY/1f4NjrDjxjRHP3NXiB0AbfUc2x4w603bIBzoskHC2/76VeIHahBQrZ4GghYoObuIu+f+xNg/LuL3CzbRvX0EH985gXMGewbe2vuDvVqtun0+fcvhO/nxeduDZtDFtvG112TbGJufWne7de/a2kKfs+3gXo0NlTD8KjtU8t7vD39t/Xt26OWBF3l/oO17277qM15v3pNt1QiX0548fMhjpdRhNBC0MLvSC5jxwjIWbkrlirHd+Pyucbw/zcVJVTOCGWNHvOx2ij2RBgQdfgNVxnY769PoG+0Vsogdk728FF6fZqfKA9vH/8NbbR/2y/575BmxBlxghwpY/d+6642xgaDX5PrH2m+ICJz7VzsDU3OacLcdO3+ETnanlDc0ELQgWw/mc9mLP+KuqGTebafw2PSB9MtaYgdBW/+e3Shjmx32t/sp9q7O2D529qLalr9gc+O1p/1r3xuumGtHhPzvhbYv/nvX2N40s97yblrE4HBbu9j0ke0hVCV5pb17dPARJuw+XiJibbdSpZRXNBC0EGuScpg5exmBATD35vH07+SZsfPgevv38wftfKpVaZnup9i/HfrXDQQleYffB1Cl9xlw+492rPrdSyG6m+0PH9ro7KB1jbjazoq1odZYgevfs6N/9pvWtINWSrUI2muoBfh4TQq/nZfIwMh8/nLTxXRrXytPn7ENwmPtjV5fPGTTMJHxdhx6gLj+tttnaYEdq3/L/+zdvCOvrf/DgkJh0q9sQ7MrpGnzpoJnGr/BsOI/9rPjB9gB2vpObVpAUUq1GBoI/Kiy0vDkwm38a/EO/hL7FZeVzEOiDmlsTd9qu1nGnmyn33OFwclTa3q+VA3PkL4Vuoy03TGju9WMu9OQpszCVZuIzcF/cCO8dm7N+iFN6C2klGpRNBD4iTGGBz9Yx7uJycwc1ZUZBfuQPcWQurHmJO4usUMvD7rEzqC06WPbJbTbKTU7qgoEaZtsN8ldX9u7en15S/+QGbaBd/8aOLDG1lb6aE5eqROVBgI/+dsXW3k3MZm7zjiJ+6b0Rv6y2r5wYG1NIMjaae8TiDvZpnEueM7O51q7ITSmh60lpG+xKSFTeXwabSM72HJoo6xSJzwNBH7w2ve7+ffXdtKY+87qi6RvseO/AxxcV7Nh+lb7t2rkxq6j4Rer6+4sINBOqZi2yY6FEz+4/rHqlVKqAdpr6Dj78KdkHl+wibMHxPPEhYPsUNHJK+2L0d3sHMFVMrYBAu1PanynHQbAvuV24vaGRu5USqkGaCA4TiorDX//Yiv3vrOWsT3b8czlwwmsmjQmOdGOZzPgAntlX+G269O32IlcgsIa33mH/nbEUEQDgVKqyTQ1dBzkl9N/2FkAACAASURBVLi59521LNycyqzRXfndBYMIdtWKwcmJdqrFTsOgoswGgI6D7UxhsV5M6NHBM69s9wnHNmOWUsqRtEbQnPL2173jFnBXVHLtqytZsjWNx6cP5E8XD64bBErzIX0zJIyCjkPsugPr7IQymTu8m9mp4xB7J7EOqaCUOgpaI2hOr0+38/vOeste4QN//XwLq/Zm8/SsYVwwrJ6B1fb/ZHv6dBlth4EICrcNxt3G2QnUvQkEUfHwf1vt9IBKKdVEWiNoLiV5kLndjv3/6rmw5m2+3HiQl77dzVXjutcEgcpKe39AleRE+zdhhO0B1HGwrRFU9RjyJjUEdsYsnQ5QKXUUNBA0l6oT9/nP2PsAPrqVNe/9gcEJbfnNtFrdOT9/AJ4bA0VZdjk50fYKqpr6sOMQO75Qumf8oLi+x+8YlFKOpIGguVTNCdD9FCqv/IBNQQO5hEU8d8UIQly1ZgXb9Y0dqfODm2ztICXRtg9U6TTE3lOw7Qs7q1Zo2+N7HEopx9FA0FzSt9gROGN68PGGND4r6k8v9tMtorxmm9J8e29AhwGwYyEsuBsKUqvbE4CaBuOk5XbOXaWU8jENBM0lbTPE9qXQbfjzZ1vIbz8EwdixeKocWAsYOPNxGDLTTqcIdQNBh/4Q4GnDj+t33IqvlHIuDQTNJX0LdOjPv7/eQWpeKRdOm27Xp6yq2SbFMzxE5+F2GsW4/raXUPygmm1cITVDRGj7gFLqONDuo82hJBfyUsiJ6M1L3+7mouEJDOvb084ZUDsQ7P8J2naFyDi7fPXHdoL1wKC6++s41DYYe9tjSCmljoHWCJqDp8fQG7vCcAUID57jSekkjKypBQDsX21rA1Wi4u1AcofqPt62N1TdMayUUj6kgaA5eKaKfHdfJHecfhLxbTzz/yaMhLwUyDtgu4tm77H3CxzJ0Cvg7rVHP3mMUko1gaaGmkFF6mbcBBMY050bT+tZ80Jnz0l//2p7hQ91awQNCQiAqI7NX1CllKqHBoJmsH/HT+RUduaR6YPr3jPQaQhIoG0nqBpBtNMw/xRSKaUa4NPUkIhMFZGtIrJDRB6s5/VuIrJERH4SkXUicm59+2nJUvNKCMrcRn7USZzer0PdF4PCIH6gbSfYvwba9YawaP8UVCmlGuCzQCAigcBzwDnAAOByETm09fM3wLvGmOHALODfviqPrzy1YCUdJYt+QxqYLL6qwThllXftA0opdZz5skYwBthhjNlljCkD5gIXHLKNAdp4nrcF9vuwPM3uh50ZbF1vB41r12No/RsljITSXMg/4F37gFJKHWe+DAQJQFKt5WTPutoeA34uIsnAp8Bd9e1IRG4WkUQRSUxPT/dFWZustLyC33y0gXFRaXZFhwbuAk4YWfO8s9YIlFItj7+7j14OvGaM6QKcC/xXRA4rkzFmtjFmlDFmVFxc3HEvZH1e/GYXu9ILuaJnkb07uG23+jeMOxmCIkACbOOxUkq1MEcMBCJyfn0nZy+kAF1rLXfxrKvtBuBdAGPMMiAUiD2KzzqudmcU8uySHUwb0oku7j12cLiABr6igEA7llD8QAiOOK7lVEopb3hzgp8JbBeRv4pIU0ZBWwn0EZGeIhKMbQyef8g2+4ApACLSHxsIWkbupxGPfLyBkMAAHpk2ANK21IwN1JALn4eZbxyfwimlVBMdMRAYY34ODAd2Aq+JyDJPzj7qCO8rB+4EvgA2Y3sHbRSR34mIZ0Q2/g+4SUTWAm8D1xpjzDEcj8+tT87l2+0Z3H1mHzqYDCg4eOR7A9omQEyP41I+pZRqKq9uKDPG5InIPCAMuAe4CPiViDxjjPlXI+/7FNsIXHvdI7WebwImHE3B/eWtFfsIDQrgstFdYccndmXXBrqOKqXUCcCbNoLpIvIh8DUQBIwxxpwDDMVe0TtGQWk589ekcP6QzrQJDYKkFeAKs/MMK6XUCcqbGsElwJPGmKW1VxpjikTkBt8Uq2X6ZO1+CssquHysp4dQ8gp7k9ihw0grpdQJxJvG4seAFVULIhImIj0AjDGLfFKqFurtFfvo1zGK4V2jwV1sZxzTtJBS6gTnTSB4D6istVzhWecoG1JyWZecy+VjuiEiduygynLoooFAKXVi8yYQuDxDRADgeR7suyK1TG+v2EeIK4ALh3tujk5abv9qjUApdYLzJhCk1+ruiYhcAGT4rkgtT1FZOR+v2c+0IZ1pG+ZpD0haYUcTjWjx978ppVSjvGksvhV4U0SeBQQ7ftDVPi1VC7NocxoFpeVcOrKLXWGMbSg+6Uz/FkwppZrBEQOBMWYnME5EIj3LBT4vVQuzYN1+OkSFMKZnO7siezcUpmtaSCnVKnh1Q5mInAcMBEJFBABjzO98WK4WI7/EzZKt6VwxphuBAfbYSfJ0ouo61n8FU0qpZuLNDWUvYMcbugubGpoBdPdxuVqMhZtTKSuv5PyhnWtWJq2A4CiIa8rQS0op1TJ501h8ijHmaiDbGPM4MB7o69titRyfrD1AQnQYI7rVmmIyaYUdUTQgsOE3KqXUCcKbQFDi+VskIp0BN9DJd0VqOXKL3Hy7PZ3zhnSiKiXG7qWQugG6n+LfwimlVDPxpo3gExGJBv4GrMZOL/mST0vVQnyx8SDuCsP5QzxpodxkeO86O//AuNv8WzillGomjQYCz4Q0i4wxOcD7IrIACDXG5B6X0vnZJ+v20719OIMS2kB5Kbx7tf07600IaXQUbqWUOmE0mhoyxlQCz9VaLnVKEMgqLOOHnZlMq0oLfXY/pKyCi56H2D7+Lp5SSjUbb9oIFonIJVKdJHeGFbszqag0nNEvHnJTYNVrMO526H++v4umlFLNyptAcAt2kLlSEckTkXwRyfNxufxu1d5sgl0BNi2Uvtmu7DfNv4VSSikf8ObOYkcmwxP3ZjMkoS0hrkBI32pX6n0DSqlW6IiBQEQm1rf+0IlqWpMSdwUbUnK5/tSedkX6FgiPhYj2/i2YUkr5gDfdR39V63koMAZYBZzhkxK1AOtTcnFXGEZ2i7Er0rdqbUAp1Wp5kxqq0zoqIl2Bp3xWohZg1d5sAEZ2j7EjjaZvgUGX+rlUSinlG940Fh8qGejf3AVpSRL3ZNMzNoL2kSFQkAoluVojUEq1Wt60EfwLezcx2MAxDHuHcatkjGH1vmzO6NfBrkjfYv/GOWZ4JaWUw3jTRpBY63k58LYx5nsflcfvdmcUklVYZtNCoD2GlFKtnjeBYB5QYoypABCRQBEJN8YU+bZo/lHVPjCqOhBsgdC2EBnvx1IppZTveHVnMRBWazkMWOib4vjfqr3ZtAl10Tsu0q6o6jHkrBurlVIO4k0gCK09PaXnebjviuRfq/ZmM7J7DAFVs5Glb4G4k/1bKKWU8iFvAkGhiIyoWhCRkUCx74rkPzlFZWxPK6hpHyjMgKJMbR9QSrVq3rQR3AO8JyL7sVNVdsROXdnqrE+xA6sO71arfQC0RqCUatW8uaFspYj0A6rOhluNMW7fFss/tqXaDNjJHT3DK2mPIaWUA3gzef0dQIQxZoMxZgMQKSK3+75ox9+OtHxiwoNoHxFsV6RvheBIaJPg34IppZQPedNGcJNnhjIAjDHZwE2+K5L/bE8toE98VM38xOlb7LSU2mNIKdWKeRMIAmtPSiMigUCw74rkH8YYtqcV0KdDZM1KHWxOKeUA3jQWfw68IyIvepZvAT7zXZH8I72glNxitw0ElRWw62soOKgNxUqpVs+bQPAAcDNwq2d5HbbnUKuyPbWAQCo4K/lZ+Md8KEyD4Cjoc7a/i6aUUj51xNSQZwL75cAe7FwEZwCbvdm5iEwVka0iskNEHqzn9SdFZI3nsU1Ecurbz/GwPTWf0wLWkbD5P9B5GFz6KvxyK8QP8FeRlFLquGiwRiAifYHLPY8M4B0AY8zp3uzY05bwHHAWdujqlSIy3xizqWobY8y9tba/Cxh+FMfQLLanFTA4+KBduHg2hMX4qyhKKXVcNVYj2IK9+p9mjDnVGPMvoKIJ+x4D7DDG7DLGlAFzgQsa2f5y4O0m7L9ZbU8rYHjoQTu4nAYBpZSDNBYILgYOAEtE5CURmYK9s9hbCUBSreVkz7rDiEh3oCewuIHXbxaRRBFJTE9Pb0IRvGOMYXtqPicFpGjjsFLKcRoMBMaYj4wxs4B+wBLsUBMdROR5EWnuFtRZwLyqoa7rKctsY8woY8youLi4Zv5oyCwsI7uojI6le7W7qFLKcbxpLC40xrzlmbu4C/ATtifRkaQAXWstd/Gsq88s/JkWSi2gI1kEVxRqjUAp5ThNmrPYGJPtuTqf4sXmK4E+ItJTRIKxJ/v5h27kGccoBljWlLI0px1p+fQJ8MQorREopRzmaCav94oxphy4E/gC2930XWPMRhH5nYhMr7XpLGCuMcbUt5/jYVtqAYOCDtgFDQRKKYfx5oayo2aM+RT49JB1jxyy/Jgvy+CN7Wn5XB+WCgHtISLW38VRSqnjymc1ghPJjrQCmxrS2oBSyoEcHwiyCsvIKCilU9lebShWSjmS4wPBjrQCYskjtDxPawRKKUdyfCBIySmiT0CyXYjt69/CKKWUHzg+EGTkl3GSaNdRpZRzOT4QpBeU0i9wPyakDUS1utG1lVLqiBwfCDLyS+nv2o/E9dMpKZVSjuT4QJBeUEovk6w9hpRSjuX4QFCWl060ydH2AaWUYzk+ELQp2GWfaCBQSjmUowNBZaUhvDTNLkR3bXxjpZRqpRwdCLKLyogyBXYhNNq/hVFKKT9xdCBILyilDYV2IUwDgVLKmRwdCDLyy4iWQioCQ8EV4u/iKKWUXzg7EBSU0pZCjKaFlFIOpoFAChFNCymlHMzRgSC9oJRoKSQgPMbfRVFKKb9xdCDIyC+jXWAREqaBQCnlXI4OBFU1Au06qpRyMkcHgoz8UqJMoXYdVUo5mqMDQXZ+EWGmSGsESilHc2wgqKw0uIuy7YLWCJRSDubYQJBT7CZSh5dQSinnBoKqm8kArREopRzNsYEgPd/eTAZojUAp5WiODQRaI1BKKcuxgUBrBEopZTk2EGQUlNEuQGsESinl4EBQSnxQCbjCdAhqpZSjufxdAH/JKCglLqgYgrU2oJRyNkfXCNoH6F3FSinl2EBgG4uLtH1AKeV4jgwElZWGzIIyoigAHYJaKeVwjgwEucVuyisNEZUFmhpSSjmeTwOBiEwVka0iskNEHmxgm8tEZJOIbBSRt3xZnioZBaUAhJTnaWpIKeV4Pus1JCKBwHPAWUAysFJE5htjNtXapg/wEDDBGJMtIh18VZ7a0gtKCaSCoHKdlEYppXxZIxgD7DDG7DLGlAFzgQsO2eYm4DljTDaAMSbNh+WpllFQRhsdXkIppQDfBoIEIKnWcrJnXW19gb4i8r2I/CgiU+vbkYjcLCKJIpKYnp5+zAXLLXbr8BJKKeXh78ZiF9AHmAxcDrwkIoedmY0xs40xo4wxo+Li4o75Q0vKKojWGoFSSgG+DQQpQNday10862pLBuYbY9zGmN3ANmxg8KmisgqtESillIcvA8FKoI+I9BSRYGAWMP+QbT7C1gYQkVhsqmiXD8sEQLG7gnYBRXZBawRKKYfzWSAwxpQDdwJfAJuBd40xG0XkdyIy3bPZF0CmiGwClgC/MsZk+qpMVYrLyol1eQKB1giUUg7n00HnjDGfAp8esu6RWs8NcJ/ncdwUlVXQKbAYKtAagVLHyO12k5ycTElJib+LooDQ0FC6dOlCUFCQ1+9x5Oij1akh0SGolTpWycnJREVF0aNHD0TE38VxNGMMmZmZJCcn07NnT6/f5+9eQ35RXNVYrLUBpY5ZSUkJ7du31yDQAogI7du3b3LtzJmBwF1BtOhdxUo1Fw0CLcfR/BaODARFZRVEoTUCpZQChwaC4rIKooyOPKqUUuDUQOCusENQ61wESqkmKC8v93cRfMKRvYaKyioIr8zX1JBSzezxTzayaX9es+5zQOc2PHr+wCNud+GFF5KUlERJSQl33303N998M59//jkPP/wwFRUVxMbGsmjRIgoKCrjrrrtITExERHj00Ue55JJLiIyMpKCgAIB58+axYMECXnvtNa699lpCQ0P56aefmDBhArNmzeLuu++mpKSEsLAwXn31VU4++WQqKip44IEH+PzzzwkICOCmm25i4MCBPPPMM3z00UcAfPXVV/z73//mww8/bNbv6Fg5MhCUu0sJFZ2vWKnW5JVXXqFdu3YUFxczevRoLrjgAm666SaWLl1Kz549ycrKAuD3v/89bdu2Zf369QBkZ2cfcd/Jycn88MMPBAYGkpeXx7fffovL5WLhwoU8/PDDvP/++8yePZs9e/awZs0aXC4XWVlZxMTEcPvtt5Oenk5cXByvvvoq119/vU+/h6PhuEBgjMFVlgchaI1AqWbmzZW7rzzzzDPVV9pJSUnMnj2biRMnVvenb9euHQALFy5k7ty51e+LiTlyinjGjBkEBgYCkJubyzXXXMP27dsREdxud/V+b731VlwuV53Pu+qqq3jjjTe47rrrWLZsGXPmzGmmI24+jgsEpeWVtscQaI1AqVbi66+/ZuHChSxbtozw8HAmT57MsGHD2LJli9f7qN3t8tB++BEREdXPf/vb33L66afz4YcfsmfPHiZPntzofq+77jrOP/98QkNDmTFjRnWgaEkc11hc4q6grQ5BrVSrkpubS0xMDOHh4WzZsoUff/yRkpISli5dyu7duwGqU0NnnXUWzz33XPV7q1JD8fHxbN68mcrKykZz+Lm5uSQk2KlVXnvtter1Z511Fi+++GJ1g3LV53Xu3JnOnTvzxBNPcN111zXfQTcjxwUCHYJaqdZn6tSplJeX079/fx588EHGjRtHXFwcs2fP5uKLL2bo0KHMnDkTgN/85jdkZ2czaNAghg4dypIlSwD485//zLRp0zjllFPo1KlTg591//3389BDDzF8+PA6vYhuvPFGunXrxpAhQxg6dChvvVUzBfuVV15J165d6d+/v4++gWMjdty3E8eoUaNMYmJi09+44QNY9RqFlYFs372XYQE74Y4VEHdy8xdSKQfZvHlziz3BtRR33nknw4cP54Ybbjgun1ffbyIiq4wxo+rbvuUlq3ylsgLKSwgoLiKUMrJiR9Muupu/S6WUauVGjhxJREQE//jHP/xdlAY5JxAMmQFDZrB+dxaXvbiM//5sDKcFhfm7VEqpVm7VqlX+LsIROa6NoNhdAUB4cKCfS6KUUi2D8wJBmW3cCQtyTmVIKaUa47xA4KkRhGmNQCmlAAcGgqIyTQ0ppVRtjgsExWVaI1BKqdqcGwiCNBAo5USRkZH+LkKL47gW0yJ3BUGBQlCg42KgUr732YNwcH3z7rPjYDjnz827zxagvLy8xYw75LizYXFZBaFaG1Cq1XjwwQfrjB302GOP8cQTTzBlyhRGjBjB4MGD+fjjj73aV0FBQYPvmzNnTvXwEVdddRUAqampXHTRRQwdOpShQ4fyww8/sGfPHgYNGlT9vr///e889thjAEyePJl77rmHUaNG8fTTT/PJJ58wduxYhg8fzplnnklqamp1Oa677joGDx7MkCFDeP/993nllVe45557qvf70ksvce+99x7191aHMeaEeowcOdIci/vfW2vG/OGrY9qHUqrGpk2b/Pr5q1evNhMnTqxe7t+/v9m3b5/Jzc01xhiTnp5uevfubSorK40xxkRERDS4L7fbXe/7NmzYYPr06WPS09ONMcZkZmYaY4y57LLLzJNPPmmMMaa8vNzk5OSY3bt3m4EDB1bv829/+5t59NFHjTHGTJo0ydx2223Vr2VlZVWX66WXXjL33XefMcaY+++/39x99911tsvPzze9evUyZWVlxhhjxo8fb9atW1fvcdT3mwCJpoHzasuolxxHRe4KwoMdd9hKtVrDhw8nLS2N/fv3k56eTkxMDB07duTee+9l6dKlBAQEkJKSQmpqKh07dmx0X8YYHn744cPet3jxYmbMmEFsbCxQM9fA4sWLq+cXCAwMpG3btkec6KZq8DuwE97MnDmTAwcOUFZWVj13QkNzJpxxxhksWLCA/v3743a7GTx4cBO/rfo57oyoqSGlWp8ZM2Ywb948Dh48yMyZM3nzzTdJT09n1apVBAUF0aNHj8PmGKjP0b6vNpfLRWVlZfVyY3Mb3HXXXdx3331Mnz6dr7/+ujqF1JAbb7yRP/7xj/Tr169Zh7R2XhuBu1zvIVCqlZk5cyZz585l3rx5zJgxg9zcXDp06EBQUBBLlixh7969Xu2nofedccYZvPfee2RmZgI1cw1MmTKF559/HoCKigpyc3OJj48nLS2NzMxMSktLWbBgQaOfVzW3weuvv169vqE5E8aOHUtSUhJvvfUWl19+ubdfzxE5LxCUVWggUKqVGThwIPn5+SQkJNCpUyeuvPJKEhMTGTx4MHPmzKFfv35e7aeh9w0cOJBf//rXTJo0iaFDh3LfffcB8PTTT7NkyRIGDx7MyJEj2bRpE0FBQTzyyCOMGTOGs846q9HPfuyxx5gxYwYjR46sTjtBw3MmAFx22WVMmDDBqyk2veWc+Qg8pj61lK7twnnp6nqH5VZKNZHOR3B8TZs2jXvvvZcpU6Y0uE1T5yNwXo3ArTUCpdSJJycnh759+xIWFtZoEDgajmws1ruKlXK29evXV98LUCUkJITly5f7qURHFh0dzbZt23yyb2cGAq0RKNWsjDGIiL+L4bXBgwezZs0afxfDJ44m3a+pIaXUMQkNDSUzM/OoTkCqeRljyMzMJDQ0tEnvc1SNoKy8kvJKo6khpZpRly5dSE5OJj093d9FUdjA3KVLlya9x1GBoGYIakcdtlI+FRQUVH1HrDox+TQ1JCJTRWSriOwQkQfref1aEUkXkTWex42+LI/OV6yUUofz2aWxiAQCzwFnAcnAShGZb4zZdMim7xhj7vRVOWorqp6vWAOBUkpV8WWNYAywwxizyxhTBswFLvDh5x2RzleslFKH82WyPAFIqrWcDIytZ7tLRGQisA241xiTdOgGInIzcLNnsUBEth5lmWKBjKl/Ocp3n7higQx/F+I4c+IxgzOP24nHDE0/7u4NveDvVtNPgLeNMaUicgvwOnDGoRsZY2YDs4/1w0QksaFbrFszJx63E48ZnHncTjxmaN7j9mVqKAXoWmu5i2ddNWNMpjGm1LP4H2CkD8ujlFKqHr4MBCuBPiLSU0SCgVnA/NobiEinWovTgc0+LI9SSql6+Cw1ZIwpF5E7gS+AQOAVY8xGEfkddsq0+cAvRGQ6UA5kAdf6qjwex5xeOkE58bideMzgzON24jFDMx73CTcMtVJKqebluLGGlFJK1aWBQCmlHM4xgeBIw120BiLSVUSWiMgmEdkoInd71rcTka9EZLvnb/PNcddCiEigiPwkIgs8yz1FZLnn937H02GhVRGRaBGZJyJbRGSziIx3yG99r+ff9wYReVtEQlvb7y0ir4hImohsqLWu3t9WrGc8x75OREY09fMcEQhqDXdxDjAAuFxEBvi3VD5RDvyfMWYAMA64w3OcDwKLjDF9gEWe5dbmbur2OvsL8KQx5iQgG7jBL6XyraeBz40x/YCh2ONv1b+1iCQAvwBGGWMGYTuizKL1/d6vAVMPWdfQb3sO0MfzuBl4vqkf5ohAQAsc7sIXjDEHjDGrPc/zsSeGBOyxvu7Z7HXgQv+U0DdEpAtwHvZeFMTOkHIGMM+zSWs85rbAROBlAGNMmTEmh1b+W3u4gDARcQHhwAFa2e9tjFmK7UlZW0O/7QXAHGP9CEQf0jX/iJwSCOob7iLBT2U5LkSkBzAcWA7EG2MOeF46CMT7qVi+8hRwP1DpWW4P5Bhjyj3LrfH37gmkA696UmL/EZEIWvlvbYxJAf4O7MMGgFxgFa3/94aGf9tjPr85JRA4iohEAu8D9xhj8mq/Zmx/4VbTZ1hEpgFpxphV/i7LceYCRgDPG2OGA4UckgZqbb81gCcvfgE2EHYGIjg8hdLqNfdv65RAcMThLloLEQnCBoE3jTEfeFanVlUVPX/T/FU+H5gATBeRPdiU3xnY3Hm0J3UArfP3TgaSjTFVs63PwwaG1vxbA5wJ7DbGpBtj3MAH2H8Drf33hoZ/22M+vzklEBxxuIvWwJMbfxnYbIz5Z62X5gPXeJ5fA3x8vMvmK8aYh4wxXYwxPbC/62JjzJXAEuBSz2at6pgBjDEHgSQROdmzagqwiVb8W3vsA8aJSLjn33vVcbfq39ujod92PnC1p/fQOCC3VgrJO8YYRzyAc7FDXe8Efu3v8vjoGE/FVhfXAWs8j3OxOfNFwHZgIdDO32X10fFPBhZ4nvcCVgA7gPeAEH+XzwfHOwxI9PzeHwExTvitgceBLcAG4L9ASGv7vYG3sW0gbmzt74aGfltAsL0idwLrsT2qmvR5OsSEUko5nFNSQ0oppRqggUAppRxOA4FSSjmcBgKllHI4DQRKKeVwGgiUOoSIVIjImlqPZhu4TUR61B5RUqmWwGdTVSp1Ais2xgzzdyGUOl60RqCUl0Rkj4j8VUTWi8gKETnJs76HiCz2jAW/SES6edbHi8iHIrLW8zjFs6tAEXnJM6b+lyIS5reDUgoNBErVJ+yQ1NDMWq/lGmMGA89iRz0F+BfwujFmCPAm8Ixn/TPAN8aYodhxgDZ61vcBnjPGDARygEt8fDxKNUrvLFbqECJSYIyJrGf9HuAMY8wuz+B+B40x7UUkA+hkjHF71h8wxsSKSDrQxRhTWmsfPYCvjJ1cBBF5AAgyxjzh+yNTqn5aI1CqaUwDz5uitNbzCrStTvmZBgKlmmZmrb/LPM9/wI58CnAl8K3n+SLgNqieU7nt8SqkUk2hVyJKHS5MRNbUWv7cGFPVhTRGRNZhr+ov96y7CztT2K+ws4Zd51l/NzBbRG7AXvnfhh1RUqkW+s69sAAAAERJREFURdsIlPKSp41glDEmw99lUao5aWpIKaUcTmsESinlcFojUEoph9NAoJRSDqeBQCmlHE4DgVJKOZwGAqWUcrj/B9bZ79fwg3ueAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMBu2YDl_968",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "afaa6a0a-4f28-407a-9ce4-16138a70adcf"
      },
      "source": [
        "# Test the model\n",
        "score = model.evaluate(X_test, y_test, verbose=2)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 - 3s - loss: 0.3668 - accuracy: 0.9043\n",
            "Test loss: 0.366832971572876\n",
            "Test accuracy: 0.9042999744415283\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "effpZriUbwPs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6508bf55-480b-4ad0-e8c4-8183dd6dee00"
      },
      "source": [
        "# Save the trained weights in to .h5 format\n",
        "model.save_weights(\"DenseNet_CIFAR10.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDBJAKGZpKZx",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion \n",
        "1. Test accuracy is 90.42 and Train accuracy is 96.64 so there is a bit of overfitting.\n",
        "2. We have used the same architecture for DenseNet.\n",
        "3. Used Image Augmentation techniques like zoom level, shear range, rotation range, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3svR7VSwo1_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}